{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-31T08:47:54.064528Z",
     "start_time": "2025-07-31T08:47:46.188852Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# 定义预期列名\n",
    "column_names = ['timestamp', 'sensor_name', 'ax', 'ay', 'az', 'wx', 'wy', 'wz']\n",
    "\n",
    "def validate_input_data(file_path, expected_cols, skip_invalid=True):\n",
    "    \"\"\"\n",
    "    验证输入 CSV 文件的格式和完整性，特别处理时间戳格式 MM:SS.s。\n",
    "\n",
    "    参数：\n",
    "    file_path (str): 输入文件路径\n",
    "    expected_cols (list): 预期的列名列表\n",
    "    skip_invalid (bool): 是否跳过无效时间戳行（默认 True）\n",
    "\n",
    "    返回：\n",
    "    pd.DataFrame: 验证后的数据框\n",
    "    \"\"\"\n",
    "    # 1. 检查文件存在性\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"输入文件 {file_path} 不存在\")\n",
    "\n",
    "    # 2. 读取数据并验证列数\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, header=0, sep=',', engine='python')\n",
    "        if df.shape[1] != len(expected_cols):\n",
    "            raise ValueError(f\"文件 {file_path} 列数 ({df.shape[1]}) 与预期 ({len(expected_cols)}) 不符\")\n",
    "        df.columns = expected_cols\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"读取文件 {file_path} 失败: {e}\")\n",
    "\n",
    "    # 3. 检查缺失值\n",
    "    if df.isna().any().any():\n",
    "        print(f\"警告：文件 {file_path} 包含缺失值\")\n",
    "        df = df.dropna().reset_index(drop=True)\n",
    "    '''\n",
    "    # 4. 删除完全重复的行\n",
    "    initial_rows = len(df)\n",
    "    df = df.drop_duplicates().reset_index(drop=True)\n",
    "    if len(df) < initial_rows:\n",
    "        print(f\"警告：文件 {file_path} 包含 {initial_rows - len(df)} 行重复数据，已删除\")\n",
    "    '''\n",
    "    # 5. 验证时间戳格式 (MM:SS.s)\n",
    "    def parse_timestamp(ts):\n",
    "        if not isinstance(ts, str):\n",
    "            return pd.NaT\n",
    "\n",
    "        # 检查时间戳是否包含日期部分\n",
    "        date_pattern = r'\\d{4}-\\d{2}-\\d{2}'\n",
    "        if re.search(date_pattern, ts):\n",
    "            # 如果包含日期部分，直接解析\n",
    "            return pd.to_datetime(ts, errors='coerce')\n",
    "        else:\n",
    "            # 如果不包含日期部分，添加默认日期并解析 MM:SS.s 格式\n",
    "            try:\n",
    "                return pd.to_datetime(f\"1900-01-01 {ts}\", format='%Y-%m-%d %M:%S.%f', errors='coerce')\n",
    "            except ValueError:\n",
    "                return pd.NaT\n",
    "\n",
    "    # 保存原始时间戳以便调试\n",
    "    original_timestamps = df['timestamp'].copy()\n",
    "    df['timestamp'] = df['timestamp'].apply(parse_timestamp)\n",
    "\n",
    "    # 检查无效时间戳\n",
    "    invalid_rows = df[df['timestamp'].isna()]\n",
    "    if not invalid_rows.empty:\n",
    "        print(f\"警告：文件 {file_path} 包含 {len(invalid_rows)} 个无效时间戳，样例：\")\n",
    "        invalid_sample = pd.DataFrame({\n",
    "            'row_index': invalid_rows.index,\n",
    "            'original_timestamp': original_timestamps[invalid_rows.index]\n",
    "        })\n",
    "        print(invalid_sample.head(10))\n",
    "        if not skip_invalid:\n",
    "            raise ValueError(f\"文件 {file_path} 包含 {len(invalid_rows)} 个无效时间戳\")\n",
    "        else:\n",
    "            print(f\"跳过 {len(invalid_rows)} 个无效时间戳行\")\n",
    "            df = df.dropna(subset=['timestamp']).reset_index(drop=True)\n",
    "\n",
    "    # 6. 验证时间戳范围\n",
    "    if not df.empty:\n",
    "        minutes = df['timestamp'].dt.minute\n",
    "        seconds = df['timestamp'].dt.second + df['timestamp'].dt.microsecond / 1e6\n",
    "        if (minutes >= 60).any() or (seconds >= 60).any():\n",
    "            print(f\"警告：文件 {file_path} 包含异常时间戳（分钟或秒超出范围）\")\n",
    "            invalid_range = df[(minutes >= 60) | (seconds >= 60)][['timestamp']]\n",
    "            print(f\"异常时间戳样例：\\n{invalid_range.head()}\")\n",
    "            if not skip_invalid:\n",
    "                raise ValueError(f\"文件 {file_path} 包含无效时间戳范围\")\n",
    "            df = df[(minutes < 60) & (seconds < 60)].reset_index(drop=True)\n",
    "\n",
    "    # 7. 验证数值列\n",
    "    numeric_cols = expected_cols[2:]  # ax, ay, az, wx, wy, wz\n",
    "    df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "    if df[numeric_cols].isna().any().any():\n",
    "        print(f\"警告：文件 {file_path} 的数值列 {numeric_cols} 包含非数值数据\")\n",
    "        df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "    # 8. 检查时间戳递增\n",
    "    if not df.empty and not df['timestamp'].is_monotonic_increasing:\n",
    "        print(f\"警告：文件 {file_path} 的时间戳不是单调递增的，将进行排序\")\n",
    "        df = df.sort_values(by='timestamp').reset_index(drop=True)\n",
    "\n",
    "    # 9. 检查传感器名称\n",
    "    unique_sensors = df['sensor_name'].unique()\n",
    "    if len(unique_sensors) == 0:\n",
    "        raise ValueError(f\"文件 {file_path} 没有有效的传感器名称\")\n",
    "    print(f\"文件 {file_path} 包含传感器: {unique_sensors}\")\n",
    "\n",
    "    # 10. 检查数据范围（加速度 ±20g，角速度 ±2000 deg/s）\n",
    "    for col in numeric_cols:\n",
    "        max_val = df[col].abs().max()\n",
    "        threshold = 20 if col.startswith('a') else 2000\n",
    "        if max_val > threshold:\n",
    "            print(f\"警告：文件 {file_path} 的 {col} 列数据范围异常，最大值: {max_val}\")\n",
    "\n",
    "    # 11. 检查数据量\n",
    "    if len(df) < 10:\n",
    "        raise ValueError(f\"文件 {file_path} 数据量不足，仅有 {len(df)} 行\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# 单位转换函数\n",
    "def convert_units(data):\n",
    "    gyro_cols = ['wx', 'wy', 'wz']\n",
    "    data[gyro_cols] = data[gyro_cols] * np.pi / 180\n",
    "    return data\n",
    "\n",
    "# 数据处理流程\n",
    "def process_data(df):\n",
    "    df_sorted = df.sort_values(by='timestamp')\n",
    "    groups = df_sorted.groupby('sensor_name', sort=False)\n",
    "    processed = []\n",
    "    for name, group in groups:\n",
    "        try:\n",
    "            downsampled = group.iloc[::1]  # 未降采样\n",
    "            processed.append(downsampled)\n",
    "        except Exception as e:\n",
    "            print(f\"设备 {name} 处理失败: {e}\")\n",
    "            continue\n",
    "    final_df = pd.concat(processed).reset_index(drop=True)\n",
    "    final_df = final_df.sort_values(by=['sensor_name', 'timestamp']).reset_index(drop=True)\n",
    "    keep_cols = ['timestamp', 'sensor_name', 'ax', 'ay', 'az', 'wx', 'wy', 'wz']\n",
    "    return final_df[keep_cols]\n",
    "\n",
    "# 执行验证和处理\n",
    "file_path = 'pre/raw/YWQZ_728_2.csv'\n",
    "try:\n",
    "    df = validate_input_data(file_path, column_names, skip_invalid=True)\n",
    "    df = convert_units(df)\n",
    "    processed_data = process_data(df)\n",
    "    processed_data.to_csv('test.csv', index=False)\n",
    "    print(\"数据处理完成，保存为 test.csv\")\n",
    "    print(\"处理后数据样例：\")\n",
    "    print(processed_data.head(3))\n",
    "except Exception as e:\n",
    "    print(f\"数据验证或处理失败: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 第二部分：重新组织数据\n",
    "try:\n",
    "    df = pd.read_csv('test.csv', header=0)\n",
    "    if df.empty:\n",
    "        raise ValueError(\"文件 'test.csv' 为空\")\n",
    "    groups = df.groupby('sensor_name', sort=False)\n",
    "    group_names = list(groups.groups.keys())\n",
    "    if not group_names:\n",
    "        raise ValueError(\"CSV 文件中未找到任何设备分组\")\n",
    "\n",
    "    min_rows = min(len(groups.get_group(name)) for name in group_names)\n",
    "    final_result = []\n",
    "    for idx in range(min_rows):\n",
    "        row_data = []\n",
    "        for name in group_names:\n",
    "            group_df = groups.get_group(name)\n",
    "            row_data.extend(group_df.iloc[idx, 2:8].tolist())\n",
    "        final_result.append(row_data)\n",
    "\n",
    "    pd.DataFrame(final_result).to_csv('pre/test/qup02.csv', index=False, header=False)\n",
    "    print(f\"文件已成功保存，有效行数：{min_rows}\")\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"文件 'test.csv' 未找到，请检查路径是否正确\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"数据处理失败: {e}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件 pre/raw/YWQZ_728_2.csv 包含传感器: ['WTNB_BLE_06' 'WTNB_BLE_02' 'WTNB_BLE_05' 'WTNB_BLE_03' 'WTNB_BLE_01'\n",
      " 'WTNB_BLE_04']\n",
      "数据处理完成，保存为 test.csv\n",
      "处理后数据样例：\n",
      "                   timestamp  sensor_name    ax    ay    az        wx  \\\n",
      "0 2025-07-23 22:32:19.712155  WTNB_BLE_01  1.66  9.26  2.56  0.479442   \n",
      "1 2025-07-23 22:32:19.712461  WTNB_BLE_01  1.74  9.18  2.60  0.601790   \n",
      "2 2025-07-23 22:32:19.712705  WTNB_BLE_01  1.94  9.40  1.62  0.859749   \n",
      "\n",
      "         wy        wz  \n",
      "0 -0.205600  0.124617  \n",
      "1 -0.186401  0.118159  \n",
      "2 -0.153414  0.104371  \n",
      "文件已成功保存，有效行数：2280\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "特征数据处理",
   "id": "76224850a9c02e6f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T15:05:34.503915Z",
     "start_time": "2025-07-30T15:05:26.544329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from data_features import *  # 包含 featureRMS, featureMAV, featureWL, featureZC, featureSSC\n",
    "\n",
    "# 文件和标签映射\n",
    "file_labels = {\n",
    "    'data/s1/dunaid.csv': 0,\n",
    "    'data/s2/upaid.csv': 1,\n",
    "    'data/s3/qupaid.csv': 2,\n",
    "}\n",
    "\n",
    "def validate_feature_input(file_path):\n",
    "    \"\"\"\n",
    "    验证特征提取阶段的输入文件。\n",
    "\n",
    "    参数：\n",
    "    file_path (str): 输入文件路径\n",
    "\n",
    "    返回：\n",
    "    pd.DataFrame: 验证后的数据框\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"文件 {file_path} 不存在\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, header=None)\n",
    "        expected_cols = 36  # 6 传感器 * 6 列 (ax, ay, az, wx, wy, wz)\n",
    "        if df.shape[1] != expected_cols:\n",
    "            raise ValueError(f\"文件 {file_path} 列数 ({df.shape[1]}) 不等于预期 ({expected_cols})\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"读取文件 {file_path} 失败: {e}\")\n",
    "\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    if df.isna().any().any():\n",
    "        print(f\"警告：文件 {file_path} 包含非数值数据，已移除\")\n",
    "        df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "    if len(df) < 2:\n",
    "        raise ValueError(f\"文件 {file_path} 数据量不足，仅有 {len(df)} 行\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# 特征提取\n",
    "featureData = []\n",
    "labels = []\n",
    "timeWindow = 5\n",
    "strideWindow = 2\n",
    "\n",
    "for file_name, label in file_labels.items():\n",
    "    print(f\"处理文件: {file_name}\")\n",
    "    try:\n",
    "        df = validate_feature_input(file_name)\n",
    "        if df.empty:\n",
    "            print(f\"错误：文件 {file_name} 的数据框为空\")\n",
    "            continue\n",
    "\n",
    "        df_values = df.values\n",
    "        length = df_values.shape[0]\n",
    "        print(f\"文件 {file_name} 包含 {length} 行数据\")\n",
    "\n",
    "        for j in range(0, length - timeWindow + 1, strideWindow):\n",
    "            window_data = df_values[j:j + timeWindow, :]\n",
    "            if window_data.shape[0] < timeWindow:\n",
    "                print(f\"警告：文件 {file_name} 的窗口 {j} 数据不足，跳过\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                rms = featureRMS(window_data)\n",
    "                mav = featureMAV(window_data)\n",
    "                wl = featureWL(window_data)\n",
    "                zc = featureZC(window_data)\n",
    "                ssc = featureSSC(window_data)\n",
    "                featureStack = np.hstack((rms, mav, wl, zc, ssc))\n",
    "                if featureStack.shape[0] != 180:  # 5 特征 * 36 通道\n",
    "                    print(f\"警告：文件 {file_name} 窗口 {j} 特征维度 {featureStack.shape[0]} 不正确\")\n",
    "                    continue\n",
    "                featureData.append(featureStack)\n",
    "                labels.append(label)\n",
    "            except Exception as e:\n",
    "                print(f\"文件 {file_name} 的窗口 {j} 特征提取失败: {e}\")\n",
    "                continue\n",
    "    except Exception as e:\n",
    "        print(f\"处理文件 {file_name} 失败: {e}\")\n",
    "        continue\n",
    "\n",
    "featureData = np.array(featureData)\n",
    "labels = np.array(labels)\n",
    "\n",
    "if len(featureData) != len(labels):\n",
    "    raise ValueError(f\"特征数据 ({len(featureData)}) 和标签 ({len(labels)}) 长度不匹配\")\n",
    "if featureData.size == 0:\n",
    "    raise ValueError(\"没有提取到任何特征数据\")\n",
    "if labels.size == 0:\n",
    "    raise ValueError(\"没有提取到任何标签数据\")\n",
    "\n",
    "pd.DataFrame(featureData).to_csv('featuresdata.csv', index=False, header=False)\n",
    "pd.DataFrame(labels).to_csv('labelsdata.csv', index=False, header=False)\n",
    "\n",
    "print(\"Feature data shape:\", featureData.shape)\n",
    "print(\"Labels shape:\", labels.shape)"
   ],
   "id": "45e27dfa0a1057fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理文件: data/s1/dunaid.csv\n",
      "文件 data/s1/dunaid.csv 包含 15367 行数据\n",
      "处理文件: data/s2/upaid.csv\n",
      "文件 data/s2/upaid.csv 包含 9382 行数据\n",
      "处理文件: data/s3/qupaid.csv\n",
      "文件 data/s3/qupaid.csv 包含 16533 行数据\n",
      "Feature data shape: (20636, 180)\n",
      "Labels shape: (20636,)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "训练预处理",
   "id": "9c4134bf7e54a7ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T14:49:48.248830Z",
     "start_time": "2025-07-31T14:47:47.831236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# 全局超参数\n",
    "LAMBDA_LOSS_AMOUNT = 0.0015\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 24\n",
    "LEARNING_RATE = 0.001\n",
    "N_STEPS = 2\n",
    "N_SIGNALS = 90\n",
    "N_CLASSES = 3\n",
    "N_HIDDEN = 256\n",
    "TRAIN_DIR = \"train\"\n",
    "\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    自定义加权交叉熵损失函数。\n",
    "\n",
    "    参数：\n",
    "    weights: 类别权重数组，形状 (N_CLASSES,)\n",
    "    \"\"\"\n",
    "    weights = tf.constant(weights, dtype=tf.float32)\n",
    "\n",
    "    def loss(y_true, y_pred):\n",
    "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0 - 1e-7)\n",
    "        cross_entropy = -tf.reduce_sum(y_true * tf.math.log(y_pred) * weights, axis=-1)\n",
    "        return tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def LSTM_RNN(n_steps, n_input, n_hidden, n_classes, lambda_loss_amount, loss_function, weights=None):\n",
    "    \"\"\"\n",
    "    构建 LSTM 模型，支持多种损失函数。\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.Input(shape=(n_steps, n_input), name='input_layer')\n",
    "\n",
    "    lstm_layer_1 = tf.keras.layers.LSTM(\n",
    "        units=n_hidden,\n",
    "        return_sequences=True,\n",
    "        dropout=0.3,\n",
    "        recurrent_dropout=0.2,\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(lambda_loss_amount)\n",
    "    )(inputs)\n",
    "    norm_layer_1 = tf.keras.layers.LayerNormalization()(lstm_layer_1)\n",
    "\n",
    "    lstm_layer_2 = tf.keras.layers.LSTM(\n",
    "        units=n_hidden // 2,\n",
    "        return_sequences=False,\n",
    "        dropout=0.3,\n",
    "        recurrent_dropout=0.2,\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(lambda_loss_amount)\n",
    "    )(norm_layer_1)\n",
    "    norm_layer_2 = tf.keras.layers.LayerNormalization()(lstm_layer_2)\n",
    "\n",
    "    dense_layer_1 = tf.keras.layers.Dense(\n",
    "        units=n_hidden // 4,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(lambda_loss_amount)\n",
    "    )(norm_layer_2)\n",
    "    dropout_dense = tf.keras.layers.Dropout(0.3)(dense_layer_1)\n",
    "\n",
    "    output_layer = tf.keras.layers.Dense(\n",
    "        units=n_classes,\n",
    "        activation='softmax',\n",
    "        name='output_layer',\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(lambda_loss_amount)\n",
    "    )(dropout_dense)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output_layer)\n",
    "\n",
    "    if loss_function == 'weighted_categorical_crossentropy':\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "    else:\n",
    "        loss = loss_function\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "        loss=loss,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def extract_batch_size(_train, step, batch_size):\n",
    "    \"\"\"\n",
    "    从训练数据中提取指定批次大小的数据。\n",
    "    \"\"\"\n",
    "    if len(_train) < batch_size:\n",
    "        raise ValueError(f\"训练数据样本数 ({len(_train)}) 小于批次大小 ({batch_size})\")\n",
    "\n",
    "    shape = list(_train.shape)\n",
    "    shape[0] = batch_size\n",
    "    batch_s = np.empty(shape)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        index = ((step - 1) * batch_size + i) % len(_train)\n",
    "        batch_s[i] = _train[index]\n",
    "\n",
    "    return batch_s\n",
    "\n",
    "def one_hot(y_, n_classes):\n",
    "    \"\"\"\n",
    "    将标签转换为 one-hot 编码。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        y_ = y_.reshape(-1).astype(np.int32)\n",
    "        return np.eye(n_classes)[y_]\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"one-hot 编码失败: {e}\")\n",
    "\n",
    "def load_X(input_csv_file):\n",
    "    \"\"\"\n",
    "    加载特征数据并进行预处理。\n",
    "    \"\"\"\n",
    "    if not os.path.exists(input_csv_file):\n",
    "        raise FileNotFoundError(f\"文件 {input_csv_file} 不存在\")\n",
    "\n",
    "    X = pd.read_csv(input_csv_file, header=None)\n",
    "    X = X.apply(pd.to_numeric, errors='coerce')\n",
    "    if X.isna().any().any():\n",
    "        print(f\"警告：文件 {input_csv_file} 包含非数值数据，已移除\")\n",
    "        X = X.dropna().reset_index(drop=True)\n",
    "\n",
    "    if X.empty:\n",
    "        raise ValueError(f\"文件 {input_csv_file} 数据为空\")\n",
    "\n",
    "    if X.shape[1] != N_STEPS * N_SIGNALS:\n",
    "        raise ValueError(f\"特征数 {X.shape[1]} 不匹配 {N_STEPS} * {N_SIGNALS}\")\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    print(f\"特征数据形状: {X_scaled.shape}\")\n",
    "    return X_scaled, scaler\n",
    "\n",
    "def load_y(labels_csv_file):\n",
    "    \"\"\"\n",
    "    加载标签数据。\n",
    "    \"\"\"\n",
    "    if not os.path.exists(labels_csv_file):\n",
    "        raise FileNotFoundError(f\"文件 {labels_csv_file} 不存在\")\n",
    "\n",
    "    y = pd.read_csv(labels_csv_file, header=None)\n",
    "    y = y.squeeze().values\n",
    "    y = y[np.char.isdigit(y.astype(str))]\n",
    "    if len(y) == 0:\n",
    "        raise ValueError(f\"文件 {labels_csv_file} 没有有效标签\")\n",
    "\n",
    "    y = y.astype(np.int32)\n",
    "    if not np.all(np.isin(y, range(N_CLASSES))):\n",
    "        raise ValueError(f\"标签值 {np.unique(y)} 超出预期类别范围 [0, {N_CLASSES-1}]\")\n",
    "\n",
    "    return y\n",
    "\n",
    "def validate_data_shapes(X, y):\n",
    "    \"\"\"\n",
    "    验证特征和标签数据形状是否匹配。\n",
    "    \"\"\"\n",
    "    if len(X) != len(y):\n",
    "        raise ValueError(f\"特征数据 ({len(X)}) 和标签数据 ({len(y)}) 长度不匹配\")\n",
    "    print(f\"数据验证通过：{len(X)} 样本\")\n",
    "\n",
    "def reshape_data(X, n_steps, n_signals):\n",
    "    \"\"\"\n",
    "    重新组织数据形状为 (n_samples, n_steps, n_signals)。\n",
    "    \"\"\"\n",
    "    expected_features = n_steps * n_signals\n",
    "    if X.shape[1] != expected_features:\n",
    "        raise ValueError(f\"特征数 {X.shape[1]} 不匹配 {n_steps} * {n_signals}\")\n",
    "    return X.reshape(-1, n_steps, n_signals)\n",
    "\n",
    "def compute_class_weights(y):\n",
    "    \"\"\"\n",
    "    计算类权重以处理类别不平衡。\n",
    "    \"\"\"\n",
    "    class_weights = class_weight.compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.arange(N_CLASSES),\n",
    "        y=y\n",
    "    )\n",
    "    return dict(enumerate(class_weights)), class_weights\n",
    "\n",
    "def plot_training_history(history, save_dir, loss_name):\n",
    "    \"\"\"\n",
    "    绘制并保存训练过程中的损失和准确率曲线。\n",
    "    \"\"\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # 损失曲线\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'Training and Validation Loss ({loss_name})')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    loss_plot_path = os.path.join(save_dir, f'loss_{loss_name}_{timestamp}.png')\n",
    "    plt.savefig(loss_plot_path)\n",
    "    plt.close()\n",
    "    print(f\"损失曲线已保存至: {loss_plot_path}\")\n",
    "\n",
    "    # 准确率曲线\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title(f'Training and Validation Accuracy ({loss_name})')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    acc_plot_path = os.path.join(save_dir, f'accuracy_{loss_name}_{timestamp}.png')\n",
    "    plt.savefig(acc_plot_path)\n",
    "    plt.close()\n",
    "    print(f\"准确率曲线已保存至: {acc_plot_path}\")\n",
    "\n",
    "def save_scaler_params(scaler, save_dir):\n",
    "    \"\"\"\n",
    "    使用 NumPy 保存 MinMaxScaler 的最大值和最小值。\n",
    "    \"\"\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    min_path = os.path.join(save_dir, f'scaler_min_{timestamp}.npy')\n",
    "    max_path = os.path.join(save_dir, f'scaler_max_{timestamp}.npy')\n",
    "\n",
    "    np.save(min_path, scaler.data_min_)\n",
    "    np.save(max_path, scaler.data_max_)\n",
    "    print(f\"Scaler 最小值已保存至: {min_path}\")\n",
    "    print(f\"Scaler 最大值已保存至: {max_path}\")\n",
    "\n",
    "def convert_to_tflite(model_path, tflite_path):\n",
    "    \"\"\"\n",
    "    将 Keras 模型转换为 TensorFlow Lite 格式。\n",
    "\n",
    "    参数：\n",
    "    model_path (str): 已保存的 Keras 模型路径（.h5 文件）\n",
    "    tflite_path (str): 转换后的 TensorFlow Lite 模型保存路径（.tflite 文件）\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 确保模型文件存在\n",
    "        if not os.path.exists(model_path):\n",
    "            raise FileNotFoundError(f\"模型文件 {model_path} 不存在\")\n",
    "\n",
    "        # 加载 Keras 模型\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "        print(f\"成功加载模型: {model_path}\")\n",
    "\n",
    "        # 创建 TFLite 转换器\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "        # 可选：设置优化选项\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "        # 转换为 TFLite 模型\n",
    "        tflite_model = converter.convert()\n",
    "        print(\"模型转换成功\")\n",
    "\n",
    "        # 保存 TFLite 模型\n",
    "        os.makedirs(os.path.dirname(tflite_path), exist_ok=True)\n",
    "        with open(tflite_path, 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "        print(f\"TFLite 模型已保存至: {tflite_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"转换失败: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    主函数：加载数据、训练 LSTM 模型（多种损失函数）、保存模型和可视化结果。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 加载数据\n",
    "        X, scaler = load_X('featuresdata.csv')\n",
    "        y = load_y('labelsdata.csv')\n",
    "\n",
    "        # 验证数据形状\n",
    "        validate_data_shapes(X, y)\n",
    "\n",
    "        # 打印类别分布\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        print(f\"类别分布: {dict(zip(unique, counts))}\")\n",
    "\n",
    "        # 计算类权重\n",
    "        class_weights_dict, class_weights_array = compute_class_weights(y)\n",
    "        print(f\"类权重: {class_weights_dict}\")\n",
    "\n",
    "        # 转换为 one-hot 编码\n",
    "        y_one_hot = one_hot(y, N_CLASSES)\n",
    "\n",
    "        # 重新组织数据形状\n",
    "        X_reshaped = reshape_data(X, N_STEPS, N_SIGNALS)\n",
    "\n",
    "        # 划分训练、验证和测试集\n",
    "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "            X_reshaped, y_one_hot, test_size=0.15, random_state=42, stratify=y\n",
    "        )\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_temp, y_temp, test_size=0.1765, random_state=42, stratify=np.argmax(y_temp, axis=1)\n",
    "        )\n",
    "\n",
    "        # 定义损失函数列表\n",
    "        loss_functions = [\n",
    "            ('categorical_crossentropy', None),\n",
    "            ('categorical_hinge', None),\n",
    "            ('weighted_categorical_crossentropy', class_weights_array)\n",
    "        ]\n",
    "\n",
    "        # 创建时间戳子文件夹\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        save_dir = os.path.join(TRAIN_DIR, timestamp)\n",
    "\n",
    "        # 保存 scaler 参数\n",
    "        save_scaler_params(scaler, save_dir)\n",
    "\n",
    "        # 为每种损失函数训练模型\n",
    "        for loss_name, weights in loss_functions:\n",
    "            print(f\"\\n训练模型使用损失函数: {loss_name}\")\n",
    "\n",
    "            # 构建模型\n",
    "            model = LSTM_RNN(\n",
    "                n_steps=N_STEPS,\n",
    "                n_input=N_SIGNALS,\n",
    "                n_hidden=N_HIDDEN,\n",
    "                n_classes=N_CLASSES,\n",
    "                lambda_loss_amount=LAMBDA_LOSS_AMOUNT,\n",
    "                loss_function=loss_name,\n",
    "                weights=weights\n",
    "            )\n",
    "\n",
    "            # 定义回调函数\n",
    "            early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=10,\n",
    "                restore_best_weights=True\n",
    "            )\n",
    "            reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=5,\n",
    "                min_lr=1e-6\n",
    "            )\n",
    "\n",
    "            # 训练模型\n",
    "            history = model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                epochs=EPOCHS,\n",
    "                validation_data=(X_val, y_val),\n",
    "                callbacks=[early_stopping, reduce_lr],\n",
    "                class_weight=class_weights_dict if loss_name != 'weighted_categorical_crossentropy' else None,\n",
    "                verbose=1\n",
    "            )\n",
    "\n",
    "            # 可视化训练效果\n",
    "            plot_training_history(history, save_dir, loss_name)\n",
    "\n",
    "            # 评估模型\n",
    "            test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "            print(f\"测试集损失 ({loss_name}): {test_loss:.4f}, 测试集准确率: {test_accuracy:.4f}\")\n",
    "\n",
    "            # 生成分类报告\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "            y_test_classes = np.argmax(y_test, axis=1)\n",
    "            print(f\"\\n分类报告 ({loss_name}):\")\n",
    "            print(classification_report(y_test_classes, y_pred_classes, target_names=['Class 0', 'Class 1', 'Class 2']))\n",
    "\n",
    "            # 保存模型\n",
    "            model_path = os.path.join(save_dir, f'lstm_model_{loss_name}.h5')\n",
    "            model.save(model_path)\n",
    "            print(f\"模型已保存至: {model_path}\")\n",
    "            converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "            converter.target_spec.supported_ops = [\n",
    "        tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n",
    "    ]\n",
    "            tflite_model = converter.convert()\n",
    "\n",
    "            # 保存转换后的 TensorFlow Lite 模型\n",
    "            with open('my_model.tflite', 'wb') as f:\n",
    "                f.write(tflite_model)\n",
    "            print(\"TensorFlow Lite 模型已保存为 my_model.tflite\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"训练失败: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# 部署端归一化示例\n",
    "def deploy_normalize(X, min_path, max_path):\n",
    "    \"\"\"\n",
    "    在部署端使用保存的最大值和最小值进行归一化。\n",
    "    \"\"\"\n",
    "    data_min = np.load(min_path)\n",
    "    data_max = np.load(max_path)\n",
    "\n",
    "    denominator = data_max - data_min\n",
    "    denominator[denominator == 0] = 1\n",
    "    X_scaled = (X - data_min) / denominator\n",
    "    return X_scaled"
   ],
   "id": "c7a80b155045c122",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征数据形状: (20636, 180)\n",
      "数据验证通过：20636 样本\n",
      "类别分布: {0: 7682, 1: 4689, 2: 8265}\n",
      "类权重: {0: 0.8954265382278921, 1: 1.4669794554631408, 2: 0.8322645694696511}\n",
      "Scaler 最小值已保存至: train\\20250731_224748\\scaler_min_20250731_224748.npy\n",
      "Scaler 最大值已保存至: train\\20250731_224748\\scaler_max_20250731_224748.npy\n",
      "\n",
      "训练模型使用损失函数: categorical_crossentropy\n",
      "Epoch 1/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 9ms/step - accuracy: 0.7085 - loss: 1.3666 - val_accuracy: 0.9567 - val_loss: 0.5711 - learning_rate: 0.0010\n",
      "Epoch 2/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 7ms/step - accuracy: 0.8877 - loss: 0.6864 - val_accuracy: 0.9706 - val_loss: 0.4025 - learning_rate: 0.0010\n",
      "Epoch 3/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 7ms/step - accuracy: 0.9150 - loss: 0.5006 - val_accuracy: 0.9713 - val_loss: 0.3099 - learning_rate: 0.0010\n",
      "Epoch 4/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.9119 - loss: 0.4247 - val_accuracy: 0.9729 - val_loss: 0.2580 - learning_rate: 0.0010\n",
      "Epoch 5/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.9276 - loss: 0.3510 - val_accuracy: 0.9797 - val_loss: 0.2054 - learning_rate: 0.0010\n",
      "Epoch 6/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.9301 - loss: 0.3234 - val_accuracy: 0.9845 - val_loss: 0.1730 - learning_rate: 0.0010\n",
      "Epoch 7/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.9361 - loss: 0.2848 - val_accuracy: 0.9832 - val_loss: 0.1622 - learning_rate: 0.0010\n",
      "Epoch 8/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.9360 - loss: 0.2686 - val_accuracy: 0.9900 - val_loss: 0.1483 - learning_rate: 0.0010\n",
      "Epoch 9/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 7ms/step - accuracy: 0.9414 - loss: 0.2589 - val_accuracy: 0.9816 - val_loss: 0.1569 - learning_rate: 0.0010\n",
      "Epoch 10/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.9411 - loss: 0.2466 - val_accuracy: 0.9861 - val_loss: 0.1407 - learning_rate: 0.0010\n",
      "Epoch 11/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.9449 - loss: 0.2334 - val_accuracy: 0.9845 - val_loss: 0.1279 - learning_rate: 0.0010\n",
      "Epoch 12/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.9472 - loss: 0.2272 - val_accuracy: 0.9877 - val_loss: 0.1231 - learning_rate: 0.0010\n",
      "Epoch 13/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.9354 - loss: 0.2418 - val_accuracy: 0.9919 - val_loss: 0.1218 - learning_rate: 0.0010\n",
      "Epoch 14/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.9435 - loss: 0.2263 - val_accuracy: 0.9874 - val_loss: 0.1270 - learning_rate: 0.0010\n",
      "Epoch 15/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.9484 - loss: 0.2116 - val_accuracy: 0.9877 - val_loss: 0.1226 - learning_rate: 0.0010\n",
      "Epoch 16/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.9496 - loss: 0.2016 - val_accuracy: 0.9861 - val_loss: 0.1134 - learning_rate: 0.0010\n",
      "Epoch 17/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 7ms/step - accuracy: 0.9476 - loss: 0.2124 - val_accuracy: 0.9903 - val_loss: 0.1075 - learning_rate: 0.0010\n",
      "Epoch 18/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.9492 - loss: 0.2098 - val_accuracy: 0.9900 - val_loss: 0.1053 - learning_rate: 0.0010\n",
      "Epoch 19/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.9424 - loss: 0.2130 - val_accuracy: 0.9845 - val_loss: 0.1158 - learning_rate: 0.0010\n",
      "Epoch 20/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.9516 - loss: 0.1974 - val_accuracy: 0.9874 - val_loss: 0.1041 - learning_rate: 0.0010\n",
      "Epoch 21/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 7ms/step - accuracy: 0.9470 - loss: 0.2035 - val_accuracy: 0.9797 - val_loss: 0.1253 - learning_rate: 0.0010\n",
      "Epoch 22/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.9514 - loss: 0.1986 - val_accuracy: 0.9835 - val_loss: 0.1151 - learning_rate: 0.0010\n",
      "Epoch 23/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.9461 - loss: 0.2082 - val_accuracy: 0.9913 - val_loss: 0.0980 - learning_rate: 0.0010\n",
      "Epoch 24/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 7ms/step - accuracy: 0.9464 - loss: 0.2005 - val_accuracy: 0.9922 - val_loss: 0.0956 - learning_rate: 0.0010\n",
      "损失曲线已保存至: train\\20250731_224748\\loss_categorical_crossentropy_20250731_224827.png\n",
      "准确率曲线已保存至: train\\20250731_224748\\accuracy_categorical_crossentropy_20250731_224827.png\n",
      "测试集损失 (categorical_crossentropy): 0.0992, 测试集准确率: 0.9897\n",
      "\u001B[1m97/97\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "分类报告 (categorical_crossentropy):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.98      1.00      0.99      1153\n",
      "     Class 1       1.00      0.99      1.00       703\n",
      "     Class 2       0.99      0.98      0.99      1240\n",
      "\n",
      "    accuracy                           0.99      3096\n",
      "   macro avg       0.99      0.99      0.99      3096\n",
      "weighted avg       0.99      0.99      0.99      3096\n",
      "\n",
      "模型已保存至: train\\20250731_224748\\lstm_model_categorical_crossentropy.h5\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\-\\AppData\\Local\\Temp\\tmpw849j058\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\-\\AppData\\Local\\Temp\\tmpw849j058\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\-\\AppData\\Local\\Temp\\tmpw849j058'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 2, 90), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 3), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1857294364368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1857480170896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1857294361296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1857294365712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1857294367248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1857294364560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1857294362256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1857294366864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1857298608784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1857298605136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1857298608016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1857380183376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1857380185296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1857380178384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1857380187024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1857380171856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "TensorFlow Lite 模型已保存为 my_model.tflite\n",
      "\n",
      "训练模型使用损失函数: categorical_hinge\n",
      "Epoch 1/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 7ms/step - accuracy: 0.7090 - loss: 1.2414 - val_accuracy: 0.9348 - val_loss: 0.4995 - learning_rate: 0.0010\n",
      "Epoch 2/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8816 - loss: 0.5521 - val_accuracy: 0.9545 - val_loss: 0.3290 - learning_rate: 0.0010\n",
      "Epoch 3/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8970 - loss: 0.4173 - val_accuracy: 0.9596 - val_loss: 0.2521 - learning_rate: 0.0010\n",
      "Epoch 4/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - accuracy: 0.8907 - loss: 0.3679 - val_accuracy: 0.9554 - val_loss: 0.2239 - learning_rate: 0.0010\n",
      "Epoch 5/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8994 - loss: 0.3237 - val_accuracy: 0.9545 - val_loss: 0.2036 - learning_rate: 0.0010\n",
      "Epoch 6/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - accuracy: 0.8957 - loss: 0.3016 - val_accuracy: 0.9386 - val_loss: 0.2217 - learning_rate: 0.0010\n",
      "Epoch 7/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8978 - loss: 0.2884 - val_accuracy: 0.9574 - val_loss: 0.1741 - learning_rate: 0.0010\n",
      "Epoch 8/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.9018 - loss: 0.2732 - val_accuracy: 0.9561 - val_loss: 0.1683 - learning_rate: 0.0010\n",
      "Epoch 9/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - accuracy: 0.9063 - loss: 0.2628 - val_accuracy: 0.9654 - val_loss: 0.1501 - learning_rate: 0.0010\n",
      "Epoch 10/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8987 - loss: 0.2713 - val_accuracy: 0.9570 - val_loss: 0.1648 - learning_rate: 0.0010\n",
      "Epoch 11/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.9064 - loss: 0.2517 - val_accuracy: 0.9658 - val_loss: 0.1450 - learning_rate: 0.0010\n",
      "Epoch 12/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - accuracy: 0.9098 - loss: 0.2474 - val_accuracy: 0.9360 - val_loss: 0.2043 - learning_rate: 0.0010\n",
      "Epoch 13/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - accuracy: 0.9069 - loss: 0.2507 - val_accuracy: 0.9648 - val_loss: 0.1439 - learning_rate: 0.0010\n",
      "Epoch 14/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.9130 - loss: 0.2356 - val_accuracy: 0.9661 - val_loss: 0.1418 - learning_rate: 0.0010\n",
      "Epoch 15/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.9010 - loss: 0.2571 - val_accuracy: 0.9625 - val_loss: 0.1450 - learning_rate: 0.0010\n",
      "Epoch 16/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - accuracy: 0.9131 - loss: 0.2274 - val_accuracy: 0.9677 - val_loss: 0.1317 - learning_rate: 0.0010\n",
      "Epoch 17/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.9158 - loss: 0.2276 - val_accuracy: 0.9622 - val_loss: 0.1418 - learning_rate: 0.0010\n",
      "Epoch 18/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.9062 - loss: 0.2366 - val_accuracy: 0.9283 - val_loss: 0.2146 - learning_rate: 0.0010\n",
      "Epoch 19/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - accuracy: 0.9041 - loss: 0.2435 - val_accuracy: 0.9687 - val_loss: 0.1319 - learning_rate: 0.0010\n",
      "Epoch 20/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - accuracy: 0.9105 - loss: 0.2364 - val_accuracy: 0.9703 - val_loss: 0.1293 - learning_rate: 0.0010\n",
      "Epoch 21/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.9161 - loss: 0.2187 - val_accuracy: 0.9632 - val_loss: 0.1368 - learning_rate: 0.0010\n",
      "Epoch 22/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - accuracy: 0.9117 - loss: 0.2258 - val_accuracy: 0.9629 - val_loss: 0.1350 - learning_rate: 0.0010\n",
      "Epoch 23/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - accuracy: 0.9111 - loss: 0.2259 - val_accuracy: 0.9674 - val_loss: 0.1274 - learning_rate: 0.0010\n",
      "Epoch 24/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.9117 - loss: 0.2258 - val_accuracy: 0.9680 - val_loss: 0.1276 - learning_rate: 0.0010\n",
      "损失曲线已保存至: train\\20250731_224748\\loss_categorical_hinge_20250731_224905.png\n",
      "准确率曲线已保存至: train\\20250731_224748\\accuracy_categorical_hinge_20250731_224905.png\n",
      "测试集损失 (categorical_hinge): 0.1519, 测试集准确率: 0.9545\n",
      "\u001B[1m97/97\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "分类报告 (categorical_hinge):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.93      0.96      0.94      1153\n",
      "     Class 1       1.00      0.99      0.99       703\n",
      "     Class 2       0.96      0.93      0.94      1240\n",
      "\n",
      "    accuracy                           0.95      3096\n",
      "   macro avg       0.96      0.96      0.96      3096\n",
      "weighted avg       0.95      0.95      0.95      3096\n",
      "\n",
      "模型已保存至: train\\20250731_224748\\lstm_model_categorical_hinge.h5\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\-\\AppData\\Local\\Temp\\tmp2lywb2vd\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\-\\AppData\\Local\\Temp\\tmp2lywb2vd\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\-\\AppData\\Local\\Temp\\tmp2lywb2vd'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 2, 90), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 3), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1857534013712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1857534009872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1857534012752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1857534014864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1855441916880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1855441922832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1855441923216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1855441916112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1855441921872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1855441923408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1855441916496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1855441920912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1855441920336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1855441919376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1855441920144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1855441918224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "TensorFlow Lite 模型已保存为 my_model.tflite\n",
      "\n",
      "训练模型使用损失函数: weighted_categorical_crossentropy\n",
      "Epoch 1/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 7ms/step - accuracy: 0.7179 - loss: 1.3476 - val_accuracy: 0.9577 - val_loss: 0.5625 - learning_rate: 0.0010\n",
      "Epoch 2/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8948 - loss: 0.6589 - val_accuracy: 0.9713 - val_loss: 0.3746 - learning_rate: 0.0010\n",
      "Epoch 3/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - accuracy: 0.9121 - loss: 0.4918 - val_accuracy: 0.9703 - val_loss: 0.3041 - learning_rate: 0.0010\n",
      "Epoch 4/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.9182 - loss: 0.4126 - val_accuracy: 0.9780 - val_loss: 0.2438 - learning_rate: 0.0010\n",
      "Epoch 5/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - accuracy: 0.9188 - loss: 0.3727 - val_accuracy: 0.9767 - val_loss: 0.2086 - learning_rate: 0.0010\n",
      "Epoch 6/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.9251 - loss: 0.3247 - val_accuracy: 0.9780 - val_loss: 0.1938 - learning_rate: 0.0010\n",
      "Epoch 7/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - accuracy: 0.9259 - loss: 0.3097 - val_accuracy: 0.9826 - val_loss: 0.1622 - learning_rate: 0.0010\n",
      "Epoch 8/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - accuracy: 0.9392 - loss: 0.2699 - val_accuracy: 0.9861 - val_loss: 0.1415 - learning_rate: 0.0010\n",
      "Epoch 9/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.9352 - loss: 0.2624 - val_accuracy: 0.9858 - val_loss: 0.1403 - learning_rate: 0.0010\n",
      "Epoch 10/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.9381 - loss: 0.2546 - val_accuracy: 0.9897 - val_loss: 0.1254 - learning_rate: 0.0010\n",
      "Epoch 11/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.9426 - loss: 0.2397 - val_accuracy: 0.9890 - val_loss: 0.1206 - learning_rate: 0.0010\n",
      "Epoch 12/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.9450 - loss: 0.2256 - val_accuracy: 0.9806 - val_loss: 0.1334 - learning_rate: 0.0010\n",
      "Epoch 13/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.9458 - loss: 0.2314 - val_accuracy: 0.9848 - val_loss: 0.1265 - learning_rate: 0.0010\n",
      "Epoch 14/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.9494 - loss: 0.2217 - val_accuracy: 0.9884 - val_loss: 0.1174 - learning_rate: 0.0010\n",
      "Epoch 15/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.9495 - loss: 0.2099 - val_accuracy: 0.9835 - val_loss: 0.1287 - learning_rate: 0.0010\n",
      "Epoch 16/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.9478 - loss: 0.2156 - val_accuracy: 0.9826 - val_loss: 0.1245 - learning_rate: 0.0010\n",
      "Epoch 17/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.9472 - loss: 0.2184 - val_accuracy: 0.9884 - val_loss: 0.1082 - learning_rate: 0.0010\n",
      "Epoch 18/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.9505 - loss: 0.2072 - val_accuracy: 0.9897 - val_loss: 0.1095 - learning_rate: 0.0010\n",
      "Epoch 19/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.9479 - loss: 0.2010 - val_accuracy: 0.9890 - val_loss: 0.1043 - learning_rate: 0.0010\n",
      "Epoch 20/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 8ms/step - accuracy: 0.9546 - loss: 0.1965 - val_accuracy: 0.9864 - val_loss: 0.1050 - learning_rate: 0.0010\n",
      "Epoch 21/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 7ms/step - accuracy: 0.9532 - loss: 0.1915 - val_accuracy: 0.9903 - val_loss: 0.1046 - learning_rate: 0.0010\n",
      "Epoch 22/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 7ms/step - accuracy: 0.9481 - loss: 0.2059 - val_accuracy: 0.9897 - val_loss: 0.1021 - learning_rate: 0.0010\n",
      "Epoch 23/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 7ms/step - accuracy: 0.9487 - loss: 0.1993 - val_accuracy: 0.9887 - val_loss: 0.1001 - learning_rate: 0.0010\n",
      "Epoch 24/24\n",
      "\u001B[1m226/226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 7ms/step - accuracy: 0.9557 - loss: 0.1811 - val_accuracy: 0.9855 - val_loss: 0.1070 - learning_rate: 0.0010\n",
      "损失曲线已保存至: train\\20250731_224748\\loss_weighted_categorical_crossentropy_20250731_224944.png\n",
      "准确率曲线已保存至: train\\20250731_224748\\accuracy_weighted_categorical_crossentropy_20250731_224944.png\n",
      "测试集损失 (weighted_categorical_crossentropy): 0.0978, 测试集准确率: 0.9922\n",
      "\u001B[1m97/97\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "分类报告 (weighted_categorical_crossentropy):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99      1153\n",
      "     Class 1       1.00      1.00      1.00       703\n",
      "     Class 2       0.99      0.99      0.99      1240\n",
      "\n",
      "    accuracy                           0.99      3096\n",
      "   macro avg       0.99      0.99      0.99      3096\n",
      "weighted avg       0.99      0.99      0.99      3096\n",
      "\n",
      "模型已保存至: train\\20250731_224748\\lstm_model_weighted_categorical_crossentropy.h5\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\-\\AppData\\Local\\Temp\\tmp116ahu7u\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\-\\AppData\\Local\\Temp\\tmp116ahu7u\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\-\\AppData\\Local\\Temp\\tmp116ahu7u'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 2, 90), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 3), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1855460472400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1855460472784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1855460470864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1855518292688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1855518293648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1855460467024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1855518292304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1855518293840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1855518293264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1857534010832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1857534011216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1857534014672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1857534009488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1857534015440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1857534014480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1857534013328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "TensorFlow Lite 模型已保存为 my_model.tflite\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "转化",
   "id": "5c4b50704b0cce94"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T10:45:18.296142Z",
     "start_time": "2025-07-16T10:45:18.141215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 加载H5模型\n",
    "model = tf.keras.models.load_model('lstm_model_categorical_crossentropy.h5')\n",
    "\n",
    "# 创建TFLite转换器\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "# 转换为TFLite模型\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# 保存TFLite模型\n",
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ],
   "id": "b658744e837b2332",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = 'lstm_model_categorical_crossentropy.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# 加载H5模型\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m model \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mmodels\u001B[38;5;241m.\u001B[39mload_model(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlstm_model_categorical_crossentropy.h5\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# 创建TFLite转换器\u001B[39;00m\n\u001B[0;32m      7\u001B[0m converter \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mlite\u001B[38;5;241m.\u001B[39mTFLiteConverter\u001B[38;5;241m.\u001B[39mfrom_keras_model(model)\n",
      "File \u001B[1;32m~\\.conda\\envs\\python\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:196\u001B[0m, in \u001B[0;36mload_model\u001B[1;34m(filepath, custom_objects, compile, safe_mode)\u001B[0m\n\u001B[0;32m    189\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m saving_lib\u001B[38;5;241m.\u001B[39mload_model(\n\u001B[0;32m    190\u001B[0m         filepath,\n\u001B[0;32m    191\u001B[0m         custom_objects\u001B[38;5;241m=\u001B[39mcustom_objects,\n\u001B[0;32m    192\u001B[0m         \u001B[38;5;28mcompile\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mcompile\u001B[39m,\n\u001B[0;32m    193\u001B[0m         safe_mode\u001B[38;5;241m=\u001B[39msafe_mode,\n\u001B[0;32m    194\u001B[0m     )\n\u001B[0;32m    195\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(filepath)\u001B[38;5;241m.\u001B[39mendswith((\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.h5\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.hdf5\u001B[39m\u001B[38;5;124m\"\u001B[39m)):\n\u001B[1;32m--> 196\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m legacy_h5_format\u001B[38;5;241m.\u001B[39mload_model_from_hdf5(\n\u001B[0;32m    197\u001B[0m         filepath, custom_objects\u001B[38;5;241m=\u001B[39mcustom_objects, \u001B[38;5;28mcompile\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mcompile\u001B[39m\n\u001B[0;32m    198\u001B[0m     )\n\u001B[0;32m    199\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(filepath)\u001B[38;5;241m.\u001B[39mendswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.keras\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m    200\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    201\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFile not found: filepath=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfilepath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    202\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease ensure the file is an accessible `.keras` \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    203\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mzip file.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    204\u001B[0m     )\n",
      "File \u001B[1;32m~\\.conda\\envs\\python\\Lib\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py:116\u001B[0m, in \u001B[0;36mload_model_from_hdf5\u001B[1;34m(filepath, custom_objects, compile)\u001B[0m\n\u001B[0;32m    114\u001B[0m opened_new_file \u001B[38;5;241m=\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(filepath, h5py\u001B[38;5;241m.\u001B[39mFile)\n\u001B[0;32m    115\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m opened_new_file:\n\u001B[1;32m--> 116\u001B[0m     f \u001B[38;5;241m=\u001B[39m h5py\u001B[38;5;241m.\u001B[39mFile(filepath, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    117\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    118\u001B[0m     f \u001B[38;5;241m=\u001B[39m filepath\n",
      "File \u001B[1;32m~\\.conda\\envs\\python\\Lib\\site-packages\\h5py\\_hl\\files.py:561\u001B[0m, in \u001B[0;36mFile.__init__\u001B[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001B[0m\n\u001B[0;32m    552\u001B[0m     fapl \u001B[38;5;241m=\u001B[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001B[0;32m    553\u001B[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001B[0;32m    554\u001B[0m                      alignment_threshold\u001B[38;5;241m=\u001B[39malignment_threshold,\n\u001B[0;32m    555\u001B[0m                      alignment_interval\u001B[38;5;241m=\u001B[39malignment_interval,\n\u001B[0;32m    556\u001B[0m                      meta_block_size\u001B[38;5;241m=\u001B[39mmeta_block_size,\n\u001B[0;32m    557\u001B[0m                      \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    558\u001B[0m     fcpl \u001B[38;5;241m=\u001B[39m make_fcpl(track_order\u001B[38;5;241m=\u001B[39mtrack_order, fs_strategy\u001B[38;5;241m=\u001B[39mfs_strategy,\n\u001B[0;32m    559\u001B[0m                      fs_persist\u001B[38;5;241m=\u001B[39mfs_persist, fs_threshold\u001B[38;5;241m=\u001B[39mfs_threshold,\n\u001B[0;32m    560\u001B[0m                      fs_page_size\u001B[38;5;241m=\u001B[39mfs_page_size)\n\u001B[1;32m--> 561\u001B[0m     fid \u001B[38;5;241m=\u001B[39m make_fid(name, mode, userblock_size, fapl, fcpl, swmr\u001B[38;5;241m=\u001B[39mswmr)\n\u001B[0;32m    563\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(libver, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m    564\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_libver \u001B[38;5;241m=\u001B[39m libver\n",
      "File \u001B[1;32m~\\.conda\\envs\\python\\Lib\\site-packages\\h5py\\_hl\\files.py:235\u001B[0m, in \u001B[0;36mmake_fid\u001B[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001B[0m\n\u001B[0;32m    233\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m swmr \u001B[38;5;129;01mand\u001B[39;00m swmr_support:\n\u001B[0;32m    234\u001B[0m         flags \u001B[38;5;241m|\u001B[39m\u001B[38;5;241m=\u001B[39m h5f\u001B[38;5;241m.\u001B[39mACC_SWMR_READ\n\u001B[1;32m--> 235\u001B[0m     fid \u001B[38;5;241m=\u001B[39m h5f\u001B[38;5;241m.\u001B[39mopen(name, flags, fapl\u001B[38;5;241m=\u001B[39mfapl)\n\u001B[0;32m    236\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr+\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    237\u001B[0m     fid \u001B[38;5;241m=\u001B[39m h5f\u001B[38;5;241m.\u001B[39mopen(name, h5f\u001B[38;5;241m.\u001B[39mACC_RDWR, fapl\u001B[38;5;241m=\u001B[39mfapl)\n",
      "File \u001B[1;32mh5py\\\\_objects.pyx:54\u001B[0m, in \u001B[0;36mh5py._objects.with_phil.wrapper\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mh5py\\\\_objects.pyx:55\u001B[0m, in \u001B[0;36mh5py._objects.with_phil.wrapper\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mh5py\\\\h5f.pyx:102\u001B[0m, in \u001B[0;36mh5py.h5f.open\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = 'lstm_model_categorical_crossentropy.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
